# -*- coding: utf-8 -*-
"""Copy of Lezione7_LinearSVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xq2qcS9fNJpimSEcXTTMgbGQMUkx5dXJ
"""

'''from google.colab import drive
drive.mount('/content/drive')'''

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy
import sklearn
print(numpy.__version__)
print(sklearn.__version__)
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.svm import LinearSVC, SVC
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import precision_score, accuracy_score, classification_report, confusion_matrix

# Load CSV file with raw string (Windows path)
data = pd.read_csv(r"C:\Users\Maboi\OneDrive\02_Education\02_ProCert\Formatemp\Esis\IAML\bank-additional-full.csv", sep=';')

# Drop the 'duration' column as recommended by dataset description
data = data.drop('duration', axis=1)

# Preview the first 10 rows
print(data.head(10))


data.shape

data.columns

len(data.columns)

data.info()

data.describe().T

print('Jobs:\n', data['job'].unique())

print('Marital:\n', data['marital'].unique())

print('Education:\n', data['education'].unique())

print('Default:\n', data['default'].unique())
print('Housing:\n', data['housing'].unique())
print('Loan:\n', data['loan'].unique())

len(data[data['education']=='unknown'])

len(data[data['housing']=='unknown'])

len(data[data['loan']=='unknown'])

len(data[data['default']=='unknown'])

print(len(data[data['marital']=='unknown']))
print(len(data[data['job']=='unknown']))

datagrouped = data.groupby('y')

def plot_barh(array,incrementer, bias, text_color ='blue', palette_style = 'darkgrid',palette_color = 'RdBu'):

    sns.set_style(palette_style)
    sns.set_palette(palette_color)

    plt.barh(array.index, width = array.values, height = .5)
    plt.yticks(np.arange(len(array)))
    plt.xticks( range(0, round(max(array)) +bias, incrementer ))

    for index, value in enumerate(array.values):
        plt.text(value +.5, index, s= '{:.1f}%'.format(value), color = text_color)

    #plt.show()
    return plt

def feature_perc(feature,groupby= 'yes'):

    count = datagrouped.get_group(groupby)[feature].value_counts()
    total_count = data[feature].value_counts()[count.index]

    perc = (count/total_count)*100
    return perc

obj_column = data.dtypes[data.dtypes == 'object'].index
obj_column

for column in obj_column[:-1]:

    yes_perc = feature_perc(column, groupby='yes')
    no_perc = feature_perc(column, groupby='no')

    plt.figure(figsize=(16,6))

    plt.subplot(1,2,1)
    plt.title(f'Success rate by  {column}')
    plot_barh(yes_perc.sort_values(),5,10)

    plt.subplot(1,2,2)
    plt.title(f'Failure rate by  {column}')
    plot_barh(no_perc.sort_values(),5,10)
    plt.show()
    print()

int_column = data.dtypes[data.dtypes == 'int64'].index.union(data.dtypes[data.dtypes == 'float64'].index)

for column in int_column:
    plt.figure(figsize=(16,4))

    plt.subplot(1,3,1)
    sns.distplot(data[column])
    plt.xlabel(column)
    plt.ylabel('Density')
    plt.title(f'{column}  Distribution')

    plt.subplot(1,3,2)
    sns.boxplot(x='y', y=column, data =data, showmeans=True )
    plt.xlabel('Target')
    plt.ylabel(column)
    plt.title(f'{column}  Distribution')

    plt.subplot(1,3,3)
    counts, bins = np.histogram(data[column], bins=20, density=True)
    cdf = np.cumsum (counts)
    plt.plot (bins[1:], cdf/cdf[-1])
    #plt.xticks(range(15,100,5))
    plt.yticks(np.arange(0,1.1,.1))
    plt.title(f'{column}  cdf')
    plt.show()
    print()

# Trasforma tutte le variabili categoriche in dummy variables (0/1)
encoded_data = pd.get_dummies(data, drop_first=True)
# drop_first=True evita collinearità eliminando una colonna per ogni categoria

# Calcola la matrice di correlazione
corr = encoded_data.corr()

# Heatmap
plt.figure(figsize=(21,12))
sns.heatmap(corr, annot=False, cmap='RdYlGn')  # annot=False perché con tante colonne diventa illeggibile
plt.show()

df = data.copy()

df['poutcome'] = df['poutcome'].map({'failure': -1,'nonexistent': 0,'success': 1})
df['default'] = df['default'].map({'yes': -1,'unknown': 0,'no': 1})
df['housing'] = df['housing'].map({'yes': -1,'unknown': 0,'no': 1})
df['loan'] = df['loan'].map({'yes': -1,'unknown': 0,'no': 1})

nominal = ['job','marital','education','contact','month','day_of_week']
dataProcessed = pd.get_dummies(df,columns=nominal)

dataProcessed['y']=dataProcessed['y'].map({'yes': 1,'no': 0})
dataProcessed.head()

import seaborn as sns
import matplotlib.pyplot as plt

# Calcola matrice di correlazione
corr = dataProcessed.corr()

# Clustermap senza numeri
sns.clustermap(corr,
                annot=False,     # niente numeri
                cmap="RdYlGn",
                center=0,
                figsize=(18, 10),
                cbar_kws={"shrink": 0.8})

plt.suptitle("Matrice di Correlazione con Clustering", fontsize=16, y=1.02)
plt.show()
#Copia
df = data.copy()

df['poutcome'] = df['poutcome'].map({'failure': -1,'nonexistent': 0,'success': 1})
df['default'] = df['default'].map({'yes': -1,'unknown': 0,'no': 1})
df['housing'] = df['housing'].map({'yes': -1,'unknown': 0,'no': 1})
df['loan'] = df['loan'].map({'yes': -1,'unknown': 0,'no': 1})

nominal = ['job','marital','education','contact','month','day_of_week']
dataProcessed = pd.get_dummies(df,columns=nominal)

dataProcessed['y']=dataProcessed['y'].map({'yes': 1,'no': 0})
dataProcessed.head()

#age
Q1 = df['age'].quantile(.20)
Q3 = df['age'].quantile(.80)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
df = df[df['age'] >= lower]
df = df[df['age'] <=upper]
#previous
Q1 = df['previous'].quantile(.20)
Q3 = df['previous'].quantile(.80)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
df = df[df['previous'] >= lower]
df = df[df['previous'] <=upper]
#campaign
Q1 = df['campaign'].quantile(.20)
Q3 = df['campaign'].quantile(.80)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
df = df[df['campaign'] >= lower]
df = df[df['campaign'] <=upper]
#cons.conf.idx
Q1 = df['cons.conf.idx'].quantile(.20)
Q3 = df['cons.conf.idx'].quantile(.80)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
df = df[df['cons.conf.idx'] >= lower]
df = df[df['cons.conf.idx'] <=upper]

columns = ['cons.conf.idx','campaign','previous','age']

for column in columns:
  sns.boxplot(x='y', y=column, data =df, showmeans=True )
  plt.xlabel('Target')
  plt.ylabel(column)
  plt.title(f'{column}  Distribution')
  plt.show()

X = dataProcessed.drop('y', axis=1).values
y = dataProcessed['y'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)


print('X train size: ', X_train.shape)
print('y train size: ', y_train.shape)
print('X test size: ', X_test.shape)
print('y test size: ', y_test.shape)

idx_numeric=[0,7,8,9,11,10,11,12,13]
print(dataProcessed.columns[idx_numeric])

# standardize numeric variables only
scaler = StandardScaler()
X_train[:,idx_numeric]=scaler.fit_transform(X_train[:,idx_numeric])
X_test[:,idx_numeric]=scaler.transform(X_test[:,idx_numeric])

tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.1],
                    'C': [1]},
                    {'kernel': ['linear'], 'C': [1]}]

clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='precision')
clf.fit(X_train, y_train)

print(clf.cv_results_)

print('The best model is: ', clf.best_params_)
print('This model produces a mean cross-validated score (precision) of', clf.best_score_)

y_pred = clf.predict(X_test)

print(classification_report(y_test,y_pred))

cnf_matrix = confusion_matrix(y_test, y_pred)
p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')